<div align="center">
  <img src="img/logo.png" alt="Agent2Bench Logo" width="200"/>
</div>

# Agent2Bench

Agent2Bench is a benchmark for testing LLM real-world capabilities.
A demo can be found at [https://dkealvaro.github.io/Agent2Bench/](https://dkealvaro.github.io/Agent2Bench/). Note that the demo doesn't include real results, it's just a proof of concept.

## Motivation
I have a vision of a future where LLMS are able to use a computer's full capabilities, just in the same way a human would do:
- Receive a natural language instruction of a task to complete with a computer (e.g. "Book a flight to Tokyo")
- Come up with a plan to complete the task with a computer, without the need of specific API or similar tools, simply by inspecting the computer's screen and using the keyboard and mouse (e.g. "Open a browser, go to Google Flights, search for flights to Tokyo, book the cheapest flight")
- Execute the plan step by step, using the computer's full capabilities (e.g. "Click on the search button, select the cheapest flight, click on the book button")
Currently, no benchmark that measures this capability exists. Agent2Bench is my attempt to fill this gap.

Such a benchmark should be as open as possible, therefore it should allow anyone to submit tasks to be benchmarked, and to view the results.

## Collaboration
I'm looking for collaborators to help me build this benchmark. If you're interested in collaborating, please contact me.














